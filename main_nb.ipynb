{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7e1e7665",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install tensorflow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fc4ff7aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import h5py\n",
    "import plotly\n",
    "#import pyyaml\n",
    "import matplotlib.pyplot as plt\n",
    "from plotly.subplots import make_subplots\n",
    "import plotly.graph_objects as go\n",
    "import seaborn as sns\n",
    "import os\n",
    "import numpy as np\n",
    "import timeit\n",
    "import time\n",
    "import tensorflow as tf\n",
    "import tensorflow.keras\n",
    "import pickle as pkl\n",
    "from tensorflow.keras.layers import LSTM, Bidirectional\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import BatchNormalization\n",
    "from tensorflow.keras.layers import Conv2D, Conv1D, MaxPooling1D\n",
    "from tensorflow.keras.layers import MaxPooling2D\n",
    "from tensorflow.keras.layers import Activation\n",
    "from tensorflow.keras.layers import Dropout\n",
    "from tensorflow.keras.layers import Dense\n",
    "from tensorflow.keras.layers import Flatten\n",
    "from tensorflow.keras.layers import Input\n",
    "from tensorflow.keras.layers import Attention\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras import backend as K\n",
    "import tensorflow_probability as tfp\n",
    "#from tensorflow.keras import mixed_precision\n",
    "from tensorflow.keras.callbacks import ReduceLROnPlateau\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "#tf.compat.v1.disable_eager_execution()\n",
    "np.set_printoptions(suppress=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d33eb381",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(tf.__version__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e647b8e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "!nvidia-smi -L"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "10f851e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "%run metadata_proc.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bed5950a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read metadata #\n",
    "\n",
    "dloc = '/Stanford_Dataset/chunk2.csv'\n",
    "wloc = '/Stanford_Dataset/chunk2.hdf5'\n",
    "d1, b = mda_proc(dloc)\n",
    "w, m, wlen, mlen = wave_proc(d1, wloc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a933eb6a",
   "metadata": {},
   "outputs": [],
   "source": [
    "p1 = mag_plot(m)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0328ddfd",
   "metadata": {},
   "outputs": [],
   "source": [
    "p2 = snr_plot(d1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fadc6d74",
   "metadata": {},
   "outputs": [],
   "source": [
    "%run trace_viz.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "96d6e5ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Time domain plots #\n",
    "\n",
    "tp = tplot(w)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8fc486ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "%run hann_taper.py\n",
    "%run sigproc.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5230caf1",
   "metadata": {},
   "outputs": [],
   "source": [
    "ht = myHanningTaper(w)\n",
    "fq = fconv(ht, b)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bc127f5c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Frequency domain plots #\n",
    "\n",
    "fp = fplot(fq)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d4a7aa7c",
   "metadata": {},
   "outputs": [],
   "source": [
    "tf.keras.backend.clear_session()\n",
    "tf.random.set_seed(42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "29e059f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "%run layers_class.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c920af2d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train Test Split #\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "x1 = fq[:,:,0]\n",
    "x2 = fq[:,:,1]\n",
    "x3 = fq[:,:,2]\n",
    "\n",
    "train_e, test_e, ytrain_e, ytest_e = train_test_split(x1, m, shuffle=True, test_size=0.1)\n",
    "train_n, test_n, ytrain_n, ytest_n = train_test_split(x2, m, shuffle=True, test_size=0.1)\n",
    "train_z, test_z, ytrain_z, ytest_z = train_test_split(x3, m, shuffle=True, test_size=0.1)\n",
    "\n",
    "h = len(d1) - 0.1 * len(d1)\n",
    "\n",
    "train_e = np.reshape(train_e, (0.1*len(d1),3001,1))\n",
    "test_e = np.reshape(test_e, (h,3001,1))\n",
    "train_n = np.reshape(train_n, (0.1*len(d1),3001,1))\n",
    "test_n = np.reshape(test_n, (h,3001,1))\n",
    "train_z = np.reshape(train_z, (0.1*len(d1),3001,1))\n",
    "test_z = np.reshape(test_z, (h,3001,1))\n",
    "\n",
    "print(train_e.shape, ytrain_e.shape, test_e.shape, ytest_e.shape)\n",
    "print(train_n.shape, ytrain_n.shape, test_n.shape, ytest_n.shape)\n",
    "print(train_z.shape, ytrain_z.shape, test_z.shape, ytest_z.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "42bfc92e",
   "metadata": {},
   "source": [
    "# Training channel E"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9f3e17ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "epochs_number = 100\n",
    "bach_size = 10\n",
    "drop_rate = 0.2\n",
    "\n",
    "#strategy = tf.distribute.MirroredStrategy()\n",
    "\n",
    "#with strategy.scope():\n",
    "\n",
    "inp = Input(shape=(3001,1), dtype='float32', name='input_layer') \n",
    "#inp = tf.cast(inp, tf.int32)\n",
    "# Embed frequency data #\n",
    "#freq_embed = F2V(750,3001)(inp)\n",
    "#freq_embed = tf.cast(freq_embed, tf.float32)\n",
    "#print(K.get_value(e[:50]))\n",
    "encoder = Bidirectional(LSTM(50, return_sequences=True, dropout=0.0, recurrent_dropout=0.0))(inp)\n",
    "#print(tf.shape(e))\n",
    "#e = tf.reshape(e,(1500,200))\n",
    "e1, awts, attn_out = attention1()(encoder)\n",
    "#awts = awts.numpy()\n",
    "#print(type(a))\n",
    "#attn_out = tf.reshape(attn_out,(1500,200))\n",
    "#e = tf.expand_dims(attn_out,axis=2)\n",
    "print(tf.shape(attn_out))\n",
    "#e = Flatten()(attn_out)\n",
    "#print(tf.shape(e))\n",
    "#e = tf.reshape(e,(1,200))\n",
    "#print(tf.shape(e))\n",
    "#e = tf.expand_dims(attn_out,axis=1)\n",
    "#e = Conv1D(64,10,padding='same')(e)\n",
    "#e = Dropout(drop_rate)(e, training=True)\n",
    "#e = MaxPooling1D(4, padding='same')(e)\n",
    "#e = tf.reshape(attn_out,(200,1))\n",
    "#e = Bidirectional(LSTM(100, return_sequences=False, dropout=0.0, recurrent_dropout=0.0))(e)\n",
    "#print(tf.shape(attn_out))\n",
    "#print(tf.shape(e))\n",
    "# Downsampling with CNN\n",
    "#f = Conv1D(64, 1, padding = 'same')(inp) \n",
    "#print(tf.shape(e))\n",
    "#f = Dropout(drop_rate)(f, training=True)\n",
    "#f = MaxPooling1D(4, padding='same')(f)\n",
    "#print(tf.shape(e))\n",
    "#e = Conv1D(32, 1, padding = 'same')(f)\n",
    "#print(tf.shape(e))\n",
    "#f = Dropout(drop_rate)(f, training=True)\n",
    "#f = MaxPooling1D(4, padding='same')(f)\n",
    "#print(tf.shape(e))\n",
    "#a, attn_out = attention()(e)\n",
    "#print(tf.shape(attn_out))\n",
    "#e = tf.keras.layers.RepeatVector(188)(attn_out)\n",
    "#e = tf.expand_dims(attn_out, axis=2)\n",
    "\n",
    "# Bi-LSTM for learning temporal dependencies\n",
    "#f = Bidirectional(LSTM(100, return_sequences=False, dropout=0.0, recurrent_dropout=0.0))(f)\n",
    "\n",
    "#print(tf.shape(e))\n",
    "\n",
    "# Attention layer\n",
    "#b, a, attn_out = attention()(e)\n",
    "#print(tf.shape(attn_out))\n",
    "# Connect with a dense layer for outputting magnitude (local)\n",
    "d = Dense(1)(attn_out)\n",
    "o = Activation('linear', name='output_layer')(d)\n",
    "\n",
    "model = Model(inputs=[inp], outputs=o)\n",
    "\n",
    "model.compile(optimizer='Adam', loss='MSE')\n",
    "\n",
    "lr_reducer = ReduceLROnPlateau(monitor='val_loss',\n",
    "                               factor=np.sqrt(0.1),\n",
    "                                patience= 4,\n",
    "                               min_lr=0.5e-10\n",
    "                              )\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0cbd6391",
   "metadata": {},
   "outputs": [],
   "source": [
    "tf.keras.utils.plot_model(model,show_shapes=True,to_file='freq_mag.png')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1f6a2213",
   "metadata": {},
   "outputs": [],
   "source": [
    "start = time.perf_counter()\n",
    "history = model.fit(train_e, ytrain_e, epochs=epochs_number, validation_split=0.1, batch_size=bach_size, callbacks = [lr_reducer])\n",
    "elap = time.perf_counter() - start\n",
    "print('%.3f' % elap)\n",
    "\n",
    "# Get attention scores\n",
    "enc = model.layers[1]\n",
    "attention_layer = model.layers[2]\n",
    "encout = enc(train_e[0:1,:])\n",
    "e2, awts, _ = attention_layer(encout)\n",
    "#print(tf.reduce_max(awts),tf.reduce_min(awts))\n",
    "awts = tf.reshape(e2,(3001,1))\n",
    "train1 = tf.reshape(train_e[0:1],(3001,1))\n",
    "#print(tf.shape(awts))\n",
    "#v = yz[:1]\n",
    "#v = tf.reshape(v,(3001,1))\n",
    "#av = tf.concat((awts,train1),axis=1)\n",
    "#print(tf.shape(av))\n",
    "#merged = tf.stack((awts,train1),axis=1)\n",
    "#print(tf.shape(merged))\n",
    "fig, ax = plt.subplots(figsize=(10,10))\n",
    "#fig, ax = plt.subplots(ncols=2, gridspec_kw=dict(width_ratios=[5,0.5]))\n",
    "title = 'Attention Heatmap'\n",
    "plt.title(title,fontsize=15)\n",
    "#plt.setp(ax.get_xticklabels(), visible=False)\n",
    "ttl = ax.title\n",
    "#ax.axis('off')\n",
    "ttl.set_position([0.5,1.05])\n",
    "sns.heatmap(awts[:,:])#, annot=True, cbar=True)#, ax=axs[1])\n",
    "#fig.colorbar(axs[1].collections[0], cax=axs[1])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d80ab4c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot attention scores of respective frequencies\n",
    "\n",
    "import plotly.express as px\n",
    "sr = 100\n",
    "fny = sr / 2.0\n",
    "v = fq[0,:,0]\n",
    "m = np.linspace(0,fny,len(v))\n",
    "\n",
    "e3 = tf.reshape(e2,(3001))\n",
    "awt = tf.reshape(awts,(3001))\n",
    "#print(tf.shape(awts))\n",
    "#print(m1[:5])\n",
    "#print(awts[:5])\n",
    "#print(awts.dtype)\n",
    "#print(m1.dtype)\n",
    "#k = tf.concat([m1,awts],0)\n",
    "#k = tf.reshape(k,(3001,2))\n",
    "print(tf.shape(e3))\n",
    "e3 = e3.numpy()\n",
    "awt = awt.numpy() \n",
    "e3 = e3.reshape((1,3001))\n",
    "e3 = np.round(e3, 2)\n",
    "awt = awt.reshape((1,3001))\n",
    "#k = np.concatenate([m,e2])\n",
    "print(e3.shape,awt.shape)\n",
    "#k = k.reshape(3001,2)\n",
    "#print(k.shape)\n",
    "# Convert to dataframe\n",
    "#kp = pd.DataFrame(k, columns=['Frequency [Hz]', 'Attention Scores'])\n",
    "#print(kp.head)\n",
    "\n",
    "# Plot\n",
    "#fig = make_subplots(rows=1,cols=1,vertical_spacing=0.02,x_title='Frequency [Hz]',y_title='Attention Scores')\n",
    "#fig.add_trace(go.Histogram2d(\n",
    " #   x=m,\n",
    "  #  y=,\n",
    "   # colorscale='YlGnBu',\n",
    "#))\n",
    "\n",
    "#amp = train_loc_z[0,:,]\n",
    "mag = ytrain_e[0]\n",
    "#mag = np.repeat(mag,3001)\n",
    "#mag = np.reshape(mag, (3001,1))\n",
    "#print(amp.shape, mag.shape)\n",
    "\n",
    "#amp = amp.tolist()\n",
    "m = np.round(m, 2)\n",
    "m1 = m.tolist()\n",
    "m2 = list(map(str,m1))\n",
    "mag1 = mag.tolist()\n",
    "mag2 = list(map(str,mag1))\n",
    "print(len(m2), len(mag2))\n",
    "#fig.update_xaxes(range=range(0.0,0.005,0.001))\n",
    "#fig = px.density_heatmap(e2, x='Frequency [Hz]', y='Attention Scores', text_auto=True)\n",
    "\n",
    "fig = px.imshow(e3[:,0:10],\n",
    "                labels=dict(x='Magnitude',y='Frequency',color='Attention Scores'),\n",
    "                x=m1[:10],\n",
    "                y=mag1,\n",
    "                text_auto=True\n",
    "               )\n",
    "fig.update_xaxes(side='top')\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "181f1463",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Loss Plots #\n",
    "\n",
    "%matplotlib inline\n",
    "fig = plt.figure()\n",
    "ax = fig.add_subplot(111)\n",
    "ax.plot(history.history['loss'])\n",
    "ax.plot(history.history['val_loss'], '--')\n",
    "ax.legend(['loss', 'val_loss'], loc='upper right') \n",
    "plt.ylabel('Loss')\n",
    "plt.xlabel('Epoch')\n",
    "plt.title('E Channel Loss')\n",
    "plt.rcParams['figure.figsize'] = (5,5)\n",
    "plt.grid(b=True, which='major', color='#666666', linestyle='-')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ddcd301e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluate\n",
    "\n",
    "eva = model.evaluate(test_e, ytest_e)\n",
    "print('test loss: ',eva)\n",
    "\n",
    "predic = model.predict(test_e)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "277468be",
   "metadata": {},
   "source": [
    "# Training channel N"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8f67586d",
   "metadata": {},
   "outputs": [],
   "source": [
    "epochs_number = 100\n",
    "bach_size = 10\n",
    "drop_rate = 0.2\n",
    "\n",
    "#strategy = tf.distribute.MirroredStrategy()\n",
    "\n",
    "#with strategy.scope():\n",
    "\n",
    "inp = Input(shape=(3001,1), dtype='float32', name='input_layer') \n",
    "#inp = tf.cast(inp, tf.int32)\n",
    "# Embed frequency data #\n",
    "#freq_embed = F2V(750,3001)(inp)\n",
    "#freq_embed = tf.cast(freq_embed, tf.float32)\n",
    "#print(K.get_value(e[:50]))\n",
    "encoder = Bidirectional(LSTM(50, return_sequences=True, dropout=0.0, recurrent_dropout=0.0))(inp)\n",
    "#print(tf.shape(e))\n",
    "#e = tf.reshape(e,(1500,200))\n",
    "e1, awts, attn_out = attention1()(encoder)\n",
    "#awts = awts.numpy()\n",
    "#print(type(a))\n",
    "#attn_out = tf.reshape(attn_out,(1500,200))\n",
    "#e = tf.expand_dims(attn_out,axis=2)\n",
    "print(tf.shape(attn_out))\n",
    "#e = Flatten()(attn_out)\n",
    "#print(tf.shape(e))\n",
    "#e = tf.reshape(e,(1,200))\n",
    "#print(tf.shape(e))\n",
    "#e = tf.expand_dims(attn_out,axis=1)\n",
    "#e = Conv1D(64,10,padding='same')(e)\n",
    "#e = Dropout(drop_rate)(e, training=True)\n",
    "#e = MaxPooling1D(4, padding='same')(e)\n",
    "#e = tf.reshape(attn_out,(200,1))\n",
    "#e = Bidirectional(LSTM(100, return_sequences=False, dropout=0.0, recurrent_dropout=0.0))(e)\n",
    "#print(tf.shape(attn_out))\n",
    "#print(tf.shape(e))\n",
    "# Downsampling with CNN\n",
    "#f = Conv1D(64, 1, padding = 'same')(inp) \n",
    "#print(tf.shape(e))\n",
    "#f = Dropout(drop_rate)(f, training=True)\n",
    "#f = MaxPooling1D(4, padding='same')(f)\n",
    "#print(tf.shape(e))\n",
    "#e = Conv1D(32, 1, padding = 'same')(f)\n",
    "#print(tf.shape(e))\n",
    "#f = Dropout(drop_rate)(f, training=True)\n",
    "#f = MaxPooling1D(4, padding='same')(f)\n",
    "#print(tf.shape(e))\n",
    "#a, attn_out = attention()(e)\n",
    "#print(tf.shape(attn_out))\n",
    "#e = tf.keras.layers.RepeatVector(188)(attn_out)\n",
    "#e = tf.expand_dims(attn_out, axis=2)\n",
    "\n",
    "# Bi-LSTM for learning temporal dependencies\n",
    "#f = Bidirectional(LSTM(100, return_sequences=False, dropout=0.0, recurrent_dropout=0.0))(f)\n",
    "\n",
    "#print(tf.shape(e))\n",
    "\n",
    "# Attention layer\n",
    "#b, a, attn_out = attention()(e)\n",
    "#print(tf.shape(attn_out))\n",
    "# Connect with a dense layer for outputting magnitude (local)\n",
    "d = Dense(1)(attn_out)\n",
    "o = Activation('linear', name='output_layer')(d)\n",
    "\n",
    "model = Model(inputs=[inp], outputs=o)\n",
    "\n",
    "model.compile(optimizer='Adam', loss='MSE')\n",
    "\n",
    "lr_reducer = ReduceLROnPlateau(monitor='val_loss',\n",
    "                               factor=np.sqrt(0.1),\n",
    "                                patience= 4,\n",
    "                               min_lr=0.5e-10\n",
    "                              )\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2701b9ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "start = time.perf_counter()\n",
    "history1 = model.fit(train_n, ytrain_n, epochs=epochs_number, validation_split=0.1, batch_size=bach_size, callbacks = [lr_reducer])\n",
    "elap = time.perf_counter() - start\n",
    "print('%.3f' % elap)\n",
    "\n",
    "# Get attention scores\n",
    "enc = model.layers[1]\n",
    "attention_layer = model.layers[2]\n",
    "encout = enc(train_e[0:1,:])\n",
    "e2, awts, _ = attention_layer(encout)\n",
    "#print(tf.reduce_max(awts),tf.reduce_min(awts))\n",
    "awts = tf.reshape(e2,(3001,1))\n",
    "train1 = tf.reshape(train_e[0:1],(3001,1))\n",
    "#print(tf.shape(awts))\n",
    "#v = yz[:1]\n",
    "#v = tf.reshape(v,(3001,1))\n",
    "#av = tf.concat((awts,train1),axis=1)\n",
    "#print(tf.shape(av))\n",
    "#merged = tf.stack((awts,train1),axis=1)\n",
    "#print(tf.shape(merged))\n",
    "fig, ax = plt.subplots(figsize=(10,10))\n",
    "#fig, ax = plt.subplots(ncols=2, gridspec_kw=dict(width_ratios=[5,0.5]))\n",
    "title = 'Attention Heatmap'\n",
    "plt.title(title,fontsize=15)\n",
    "#plt.setp(ax.get_xticklabels(), visible=False)\n",
    "ttl = ax.title\n",
    "#ax.axis('off')\n",
    "ttl.set_position([0.5,1.05])\n",
    "sns.heatmap(awts[:,:])#, annot=True, cbar=True)#, ax=axs[1])\n",
    "#fig.colorbar(axs[1].collections[0], cax=axs[1])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5da5ff79",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot attention scores of respective frequencies\n",
    "\n",
    "import plotly.express as px\n",
    "sr = 100\n",
    "fny = sr / 2.0\n",
    "v = fq[0,:,1]\n",
    "m = np.linspace(0,fny,len(v))\n",
    "\n",
    "e3 = tf.reshape(e2,(3001))\n",
    "awt = tf.reshape(awts,(3001))\n",
    "#print(tf.shape(awts))\n",
    "#print(m1[:5])\n",
    "#print(awts[:5])\n",
    "#print(awts.dtype)\n",
    "#print(m1.dtype)\n",
    "#k = tf.concat([m1,awts],0)\n",
    "#k = tf.reshape(k,(3001,2))\n",
    "print(tf.shape(e3))\n",
    "e3 = e3.numpy()\n",
    "awt = awt.numpy() \n",
    "e3 = e3.reshape((1,3001))\n",
    "e3 = np.round(e3,2)\n",
    "awt = awt.reshape((1,3001))\n",
    "#k = np.concatenate([m,e2])\n",
    "print(e3.shape,awt.shape)\n",
    "#k = k.reshape(3001,2)\n",
    "#print(k.shape)\n",
    "# Convert to dataframe\n",
    "#kp = pd.DataFrame(k, columns=['Frequency [Hz]', 'Attention Scores'])\n",
    "#print(kp.head)\n",
    "\n",
    "# Plot\n",
    "#fig = make_subplots(rows=1,cols=1,vertical_spacing=0.02,x_title='Frequency [Hz]',y_title='Attention Scores')\n",
    "#fig.add_trace(go.Histogram2d(\n",
    " #   x=m,\n",
    "  #  y=,\n",
    "   # colorscale='YlGnBu',\n",
    "#))\n",
    "\n",
    "#amp = train_loc_z[0,:,]\n",
    "mag = ytrain_n[0]\n",
    "#mag = np.repeat(mag,3001)\n",
    "#mag = np.reshape(mag, (3001,1))\n",
    "#print(amp.shape, mag.shape)\n",
    "\n",
    "#amp = amp.tolist()\n",
    "m = np.round(m,2)\n",
    "m1 = m.tolist()\n",
    "m2 = list(map(str,m1))\n",
    "mag1 = mag.tolist()\n",
    "mag2 = list(map(str,mag1))\n",
    "print(len(m2), len(mag2))\n",
    "#fig.update_xaxes(range=range(0.0,0.005,0.001))\n",
    "#fig = px.density_heatmap(e2, x='Frequency [Hz]', y='Attention Scores', text_auto=True)\n",
    "\n",
    "fig = px.imshow(e3[:,0:10],\n",
    "                labels=dict(x='Magnitude',y='Frequency',color='Attention Scores'),\n",
    "                x=m1[:10],\n",
    "                y=mag1,\n",
    "                text_auto=True\n",
    "               )\n",
    "fig.update_xaxes(side='top')\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b3e8032b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Loss Plots #\n",
    "\n",
    "%matplotlib inline\n",
    "fig = plt.figure()\n",
    "ax = fig.add_subplot(111)\n",
    "ax.plot(history1.history['loss'])\n",
    "ax.plot(history1.history['val_loss'], '--')\n",
    "ax.legend(['loss', 'val_loss'], loc='upper right') \n",
    "plt.ylabel('Loss')\n",
    "plt.xlabel('Epoch')\n",
    "plt.title('N Channel Loss')\n",
    "plt.rcParams['figure.figsize'] = (5,5)\n",
    "plt.grid(b=True, which='major', color='#666666', linestyle='-')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "49872894",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluate\n",
    "\n",
    "eva1 = model.evaluate(test_n, ytest_n)\n",
    "print('test loss: ',eva1)\n",
    "\n",
    "predic1 = model.predict(test_n)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c654c0b1",
   "metadata": {},
   "source": [
    "# Training channel Z"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c4290ef0",
   "metadata": {},
   "outputs": [],
   "source": [
    "epochs_number = 100\n",
    "bach_size = 10\n",
    "drop_rate = 0.2\n",
    "\n",
    "#strategy = tf.distribute.MirroredStrategy()\n",
    "\n",
    "#with strategy.scope():\n",
    "\n",
    "inp = Input(shape=(3001,1), dtype='float32', name='input_layer') \n",
    "#inp = tf.cast(inp, tf.int32)\n",
    "# Embed frequency data #\n",
    "#freq_embed = F2V(750,3001)(inp)\n",
    "#freq_embed = tf.cast(freq_embed, tf.float32)\n",
    "#print(K.get_value(e[:50]))\n",
    "encoder = Bidirectional(LSTM(50, return_sequences=True, dropout=0.0, recurrent_dropout=0.0))(inp)\n",
    "#print(tf.shape(e))\n",
    "#e = tf.reshape(e,(1500,200))\n",
    "e1, awts, attn_out = attention1()(encoder)\n",
    "#awts = awts.numpy()\n",
    "#print(type(a))\n",
    "#attn_out = tf.reshape(attn_out,(1500,200))\n",
    "#e = tf.expand_dims(attn_out,axis=2)\n",
    "print(tf.shape(attn_out))\n",
    "#e = Flatten()(attn_out)\n",
    "#print(tf.shape(e))\n",
    "#e = tf.reshape(e,(1,200))\n",
    "#print(tf.shape(e))\n",
    "#e = tf.expand_dims(attn_out,axis=1)\n",
    "#e = Conv1D(64,10,padding='same')(e)\n",
    "#e = Dropout(drop_rate)(e, training=True)\n",
    "#e = MaxPooling1D(4, padding='same')(e)\n",
    "#e = tf.reshape(attn_out,(200,1))\n",
    "#e = Bidirectional(LSTM(100, return_sequences=False, dropout=0.0, recurrent_dropout=0.0))(e)\n",
    "#print(tf.shape(attn_out))\n",
    "#print(tf.shape(e))\n",
    "# Downsampling with CNN\n",
    "#f = Conv1D(64, 1, padding = 'same')(inp) \n",
    "#print(tf.shape(e))\n",
    "#f = Dropout(drop_rate)(f, training=True)\n",
    "#f = MaxPooling1D(4, padding='same')(f)\n",
    "#print(tf.shape(e))\n",
    "#e = Conv1D(32, 1, padding = 'same')(f)\n",
    "#print(tf.shape(e))\n",
    "#f = Dropout(drop_rate)(f, training=True)\n",
    "#f = MaxPooling1D(4, padding='same')(f)\n",
    "#print(tf.shape(e))\n",
    "#a, attn_out = attention()(e)\n",
    "#print(tf.shape(attn_out))\n",
    "#e = tf.keras.layers.RepeatVector(188)(attn_out)\n",
    "#e = tf.expand_dims(attn_out, axis=2)\n",
    "\n",
    "# Bi-LSTM for learning temporal dependencies\n",
    "#f = Bidirectional(LSTM(100, return_sequences=False, dropout=0.0, recurrent_dropout=0.0))(f)\n",
    "\n",
    "#print(tf.shape(e))\n",
    "\n",
    "# Attention layer\n",
    "#b, a, attn_out = attention()(e)\n",
    "#print(tf.shape(attn_out))\n",
    "# Connect with a dense layer for outputting magnitude (local)\n",
    "d = Dense(1)(attn_out)\n",
    "o = Activation('linear', name='output_layer')(d)\n",
    "\n",
    "model = Model(inputs=[inp], outputs=o)\n",
    "\n",
    "model.compile(optimizer='Adam', loss='MSE')\n",
    "\n",
    "lr_reducer = ReduceLROnPlateau(monitor='val_loss',\n",
    "                               factor=np.sqrt(0.1),\n",
    "                                patience= 4,\n",
    "                               min_lr=0.5e-10\n",
    "                              )\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4521cf45",
   "metadata": {},
   "outputs": [],
   "source": [
    "start = time.perf_counter()\n",
    "history2 = model.fit(train_z, ytrain_z, epochs=epochs_number, validation_split=0.1, batch_size=bach_size, callbacks = [lr_reducer])\n",
    "elap = time.perf_counter() - start\n",
    "print('%.3f' % elap)\n",
    "\n",
    "# Get attention scores\n",
    "enc = model.layers[1]\n",
    "attention_layer = model.layers[2]\n",
    "encout = enc(train_e[0:1,:])\n",
    "e2, awts, _ = attention_layer(encout)\n",
    "#print(tf.reduce_max(awts),tf.reduce_min(awts))\n",
    "awts = tf.reshape(e2,(3001,1))\n",
    "train1 = tf.reshape(train_e[0:1],(3001,1))\n",
    "#print(tf.shape(awts))\n",
    "#v = yz[:1]\n",
    "#v = tf.reshape(v,(3001,1))\n",
    "#av = tf.concat((awts,train1),axis=1)\n",
    "#print(tf.shape(av))\n",
    "#merged = tf.stack((awts,train1),axis=1)\n",
    "#print(tf.shape(merged))\n",
    "fig, ax = plt.subplots(figsize=(10,10))\n",
    "#fig, ax = plt.subplots(ncols=2, gridspec_kw=dict(width_ratios=[5,0.5]))\n",
    "title = 'Attention Heatmap'\n",
    "plt.title(title,fontsize=15)\n",
    "#plt.setp(ax.get_xticklabels(), visible=False)\n",
    "ttl = ax.title\n",
    "#ax.axis('off')\n",
    "ttl.set_position([0.5,1.05])\n",
    "sns.heatmap(awts[:,:])#, annot=True, cbar=True)#, ax=axs[1])\n",
    "#fig.colorbar(axs[1].collections[0], cax=axs[1])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c5bddb8b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot attention scores of respective frequencies\n",
    "\n",
    "import plotly.express as px\n",
    "sr = 100\n",
    "fny = sr / 2.0\n",
    "v = fq[0,:,2]\n",
    "m = np.linspace(0,fny,len(v))\n",
    "\n",
    "e3 = tf.reshape(e2,(3001))\n",
    "awt = tf.reshape(awts,(3001))\n",
    "#print(tf.shape(awts))\n",
    "#print(m1[:5])\n",
    "#print(awts[:5])\n",
    "#print(awts.dtype)\n",
    "#print(m1.dtype)\n",
    "#k = tf.concat([m1,awts],0)\n",
    "#k = tf.reshape(k,(3001,2))\n",
    "print(tf.shape(e3))\n",
    "e3 = e3.numpy()\n",
    "awt = awt.numpy() \n",
    "e3 = e3.reshape((1,3001))\n",
    "e3 = np.round(e3,2)\n",
    "awt = awt.reshape((1,3001))\n",
    "#k = np.concatenate([m,e2])\n",
    "print(e3.shape,awt.shape)\n",
    "#k = k.reshape(3001,2)\n",
    "#print(k.shape)\n",
    "# Convert to dataframe\n",
    "#kp = pd.DataFrame(k, columns=['Frequency [Hz]', 'Attention Scores'])\n",
    "#print(kp.head)\n",
    "\n",
    "# Plot\n",
    "#fig = make_subplots(rows=1,cols=1,vertical_spacing=0.02,x_title='Frequency [Hz]',y_title='Attention Scores')\n",
    "#fig.add_trace(go.Histogram2d(\n",
    " #   x=m,\n",
    "  #  y=,\n",
    "   # colorscale='YlGnBu',\n",
    "#))\n",
    "\n",
    "#amp = train_loc_z[0,:,]\n",
    "mag = ytrain_z[0]\n",
    "#mag = np.repeat(mag,3001)\n",
    "#mag = np.reshape(mag, (3001,1))\n",
    "#print(amp.shape, mag.shape)\n",
    "\n",
    "#amp = amp.tolist()\n",
    "m = np.round(m,2)\n",
    "m1 = m.tolist()\n",
    "m2 = list(map(str,m1))\n",
    "mag1 = mag.tolist()\n",
    "mag2 = list(map(str,mag1))\n",
    "print(len(m2), len(mag2))\n",
    "#fig.update_xaxes(range=range(0.0,0.005,0.001))\n",
    "#fig = px.density_heatmap(e2, x='Frequency [Hz]', y='Attention Scores', text_auto=True)\n",
    "\n",
    "fig = px.imshow(e3[:,0:10],\n",
    "                labels=dict(x='Magnitude',y='Frequency',color='Attention Scores'),\n",
    "                x=m1[:10],\n",
    "                y=mag1,\n",
    "                text_auto=True\n",
    "               )\n",
    "fig.update_xaxes(side='top')\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c87913d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Loss Plots #\n",
    "\n",
    "%matplotlib inline\n",
    "fig = plt.figure()\n",
    "ax = fig.add_subplot(111)\n",
    "ax.plot(history2.history['loss'])\n",
    "ax.plot(history2.history['val_loss'], '--')\n",
    "ax.legend(['loss', 'val_loss'], loc='upper right') \n",
    "plt.ylabel('Loss')\n",
    "plt.xlabel('Epoch')\n",
    "plt.title('Z Channel Loss')\n",
    "plt.rcParams['figure.figsize'] = (5,5)\n",
    "plt.grid(b=True, which='major', color='#666666', linestyle='-')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8ac7ea42",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluate\n",
    "\n",
    "eva2 = model.evaluate(test_z, ytest_z)\n",
    "print('test loss: ',eva1)\n",
    "\n",
    "predic2 = model.predict(test_z)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
